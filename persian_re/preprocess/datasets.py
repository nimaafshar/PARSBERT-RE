import torch
from typing import List, Dict
from transformers import BertTokenizer
import numpy as np


class PerlexDataset(torch.utils.data.Dataset):
    """PyTorch dataset for Perlex. """

    def __init__(self, tokenizer: BertTokenizer, texts: List[str], targets: List[str] = None,
                 label2id: Dict[str, int] = None,
                 max_len: int = 128):
        self.texts: List[str] = texts
        self.targets: List[str] = targets
        self.has_target: bool = isinstance(targets, list) or isinstance(targets, np.ndarray)

        self.tokenizer: BertTokenizer = tokenizer
        self.max_len: int = max_len

        self.label_map: Dict[str, int] = label2id if label2id else dict()

    def __len__(self) -> int:
        return len(self.texts)

    def __getitem__(self, item) -> Dict:
        text = str(self.texts[item])

        if self.has_target:
            target = self.label_map.get(self.targets[item], None)
            assert target is not None, f"label2id should contain target(class) name {self.targets[item]} as key"

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            truncation=True,
            max_length=self.max_len,
            return_token_type_ids=True,
            padding='max_length',
            return_attention_mask=True,
            return_tensors='pt')

        inputs = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'token_type_ids': encoding['token_type_ids'].flatten(),
        }

        if self.has_target and target:
            inputs['targets'] = torch.tensor(target, dtype=torch.long)

        return inputs


def create_data_loader(x: List[str], y: List[str], tokenizer: BertTokenizer, max_len: int, batch_size: int,
                       label2id: Dict[str, int]) -> torch.utils.data.DataLoader:
    """
    :param x: list of texts
    :param y: list of classes corresponding to elements of x
    :param tokenizer: will be used to create input (token_ids,attention_mask,token_type_ids)
    :param max_len: maximum length of sequence generated by tokenizer
    :param batch_size: used for batch data generation
    :param label2id: label to id mapping of classes
    :return: data loader object
    """
    dataset = PerlexDataset(
        texts=x,
        targets=y,
        tokenizer=tokenizer,
        max_len=max_len,
        label2id=label2id)

    return torch.utils.data.DataLoader(dataset, batch_size=batch_size)
